# -*- coding: utf-8 -*-
"""TSA_and_Anomaly_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JuRZx-mh1-o8wKpm6-ZFgf_YTx0LLX3u

# **TIME SERIES ANALYSIS AND ANOMALY DETECTION**

**Aim:**

* Time Series Analysis: Identify patterns such as trends, seasonality, and residuals in electricity demand over time.
* Anomaly Detection: Detect unusual or unexpected electricity loads that deviate significantly from the normal patterns

**About Dataset and Features**

This is an electricity load dataset (PJM_Load_MW) with hourly measurements in megawatts (MW)

---

The data has hourly timestamps (1-hour intervals) from December 30 1998 to January 2 2001

**Importing Libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats #statistic
from statsmodels.tsa.seasonal import seasonal_decompose #decompostion
from matplotlib.dates import DateFormatter
from sklearn.preprocessing import MinMaxScaler #scailing
from tensorflow.keras.models import Sequential #modeling
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import LSTM, Dense, Dropout #regularization
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

"""**Loading the dataset**"""

df= pd.read_csv('/content/PJM_Load_hourly.csv')
df.head()

df.info()

df.describe()

"""* The minimum load recorded is 17,461 MW, while the maximum load is 54,030 MW
* total 32896 counts

**Handling missing values**
"""

df.isnull().sum()

df=df.drop_duplicates()
df.info() #no duplicate values

"""As there is no null values and duplicates values

**Handle datetime formatting**
"""

df.columns = ['Datetime', 'Load_MW']

df['Datetime'] = pd.to_datetime(df['Datetime'], format='%Y-%m-%d %H:%M:%S')

# Set datetime as index
df.set_index('Datetime', inplace=True) #modifies the DataFrame directly instead of creating a new one

"""**Exploratory Time Series Analysis**"""

# Plot the time series
plt.figure(figsize=(15, 6))
plt.plot(df['Load_MW'], color='blue', linewidth=1)
plt.title('Hourly Load Over Time')
plt.xlabel('Time')
plt.ylabel('Load (MW)')
plt.grid(True) #adds grid to the plot
plt.tight_layout() #to fit everything in the plot
plt.show()

"""Since the frequency currently is in the hourly level, this will make it difficult to visualise. So we will resample it and aggregate it to a **daily level.**"""

#we can resample the data to daily level
daily_data = df.resample(rule='D').sum() #D-daily and sum the values

# Set frequency explicitly to D
daily_data = daily_data.asfreq('D')

daily_data.head()

# We can confirm it is at the right frequency
daily_data.index

# Plot the daily data
plt.figure(figsize=(12, 6))
plt.plot(daily_data.index, daily_data['Load_MW'])  # Replace 'value_column_name' with your actual column name
plt.title('Daily Data')
plt.xlabel('Date')
plt.ylabel('Values') #1e6, which means that the values on the y-axis should be multiplied by 1,000,000
plt.grid(True)
plt.show()

"""**Decomposition of a Time series Data**
---
Data = Level + Trend + Seasonality + Noise
---
To separately visualize the trend, seasonality, and residual components of the time series, we can use the seasonal_decompose function from the statsmodels library

We are decomposing based on hourly data, daily data and also Weekly data.

**1.Hourly data**
"""

from statsmodels.tsa.seasonal import seasonal_decompose

# Decompose the time series into trend, seasonality, and residual components
decomposition = seasonal_decompose(df['Load_MW'], model='additive', period=24)  #24 hours in a day

# Plot the decomposed components
plt.figure(figsize=(15, 10))

# Original data
plt.subplot(4, 1, 1) # 4 subgrid 1 column
plt.plot(df['Load_MW'], label='Original', color='blue')
plt.title('Original Time Series')
plt.legend(loc='best')

# Trend component
plt.subplot(4, 1, 2)
plt.plot(decomposition.trend, label='Trend', color='orange')
plt.title('Trend Component')
plt.legend(loc='best')

# Seasonal component
plt.subplot(4, 1, 3)
plt.plot(decomposition.seasonal, label='Seasonality', color='green')
plt.title('Seasonal Component')
plt.legend(loc='best')

# Residual component
plt.subplot(4, 1, 4)
plt.plot(decomposition.resid, label='Residuals', color='red')
plt.title('Residual Component')
plt.legend(loc='best') # to keep the plot steady

plt.tight_layout()
plt.show()
#straight line is the mean
#green visualization resolution is high, th

"""**2.Daily data**"""

# Decompose the time series into trend, seasonality, and residual components
decomposition = seasonal_decompose(daily_data['Load_MW'], model='additive', period=365)  # Assuming yearly periodicity

# Plot the decomposed components
plt.figure(figsize=(15, 10))

# Original data
plt.subplot(4, 1, 1)
plt.plot(daily_data['Load_MW'], label='Original', color='blue')
plt.title('Original Time Series')
plt.legend(loc='best')

# Trend component
plt.subplot(4, 1, 2)
plt.plot(decomposition.trend, label='Trend', color='orange')
plt.title('Trend Component')
plt.legend(loc='best')

# Seasonal component
plt.subplot(4, 1, 3)
plt.plot(decomposition.seasonal, label='Seasonality', color='green')
plt.title('Seasonal Component')
plt.legend(loc='best')

# Residual component
plt.subplot(4, 1, 4)
plt.plot(decomposition.resid, label='Residuals', color='red')
plt.title('Residual Component')
plt.legend(loc='best')

plt.tight_layout()
plt.show()

"""**3.Weekly Data**"""

# Resample the data weekly and sum the values
weekly_data = df.resample(rule='W')['Load_MW'].sum()

# Decompose the time series into trend, seasonality, and residual components
decomposition = seasonal_decompose(weekly_data, model='additive', period=52)  # Assuming weekly periodicity

# Plot the decomposed components
plt.figure(figsize=(15, 10))

# Original data
plt.subplot(4, 1, 1)
plt.plot(weekly_data, label='Original', color='blue')
plt.title('Original Time Series')
plt.legend(loc='best')

# Trend component
plt.subplot(4, 1, 2)
plt.plot(decomposition.trend, label='Trend', color='orange')
plt.title('Trend Component')
plt.legend(loc='best')

# Seasonal component
plt.subplot(4, 1, 3)
plt.plot(decomposition.seasonal, label='Seasonality', color='green')
plt.title('Seasonal Component')
plt.legend(loc='best')

# Residual component
plt.subplot(4, 1, 4)
plt.plot(decomposition.resid, label='Residuals', color='red')
plt.title('Residual Component')
plt.legend(loc='best')

plt.tight_layout()
plt.show()

"""pattern - electricity usage peak and troughs seem to be very seasonal and repetitive. This makes sense, considering office hours, weather patterns, holidays etc.

Furthermore,the trend of the data seems to be trailing upwards in the last few years.

**HeatMap**
"""

df['hour'] = df.index.hour
df['year'] = df.index.year
# Create pivot table
monthly_hourly_pivot = pd.pivot_table(df,
                                    values='Load_MW',
                                    index='hour',
                                    columns='year',
                                    aggfunc='mean')

# Plot heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(monthly_hourly_pivot,
            cmap='coolwarm', #colormap
            annot=True,
            fmt='.0f')
plt.title('Average Load by Hour and Year')
plt.xlabel('Year')
plt.ylabel('Hour of Day')
plt.show()

"""* **Daily Patterns**:the load is generally higher during the afternoon and evening hours (around 15:00 to 21:00) and lower during the early morning hours (around 0:00 to 6:00). This is consistent with typical energy usage patterns, where demand peaks in the evening when people are more likely to be home and using electricity.
* **Yearly Variations:** By comparing the columns (years), you can see how the average load changes from year to year.

# **ACF PLOT**
"""

from statsmodels.graphics.tsaplots import plot_acf

# Plot the ACF
plt.figure(figsize=(10, 6))
plot_acf(df['Load_MW'], lags=50, ax=plt.gca())
plt.title("Autocorrelation Function (ACF)")
plt.show()

"""*   ACF Plot: These ups and downs correspond to the repeating patterns due to seasonal effects.

# **Stationarity Checking Using ADF TEST** (Augmented Dickey-Fuller (ADF) test)
"""

from statsmodels.tsa.stattools import adfuller


# Perform ADF test
def perform_adf_test(series):
    result = adfuller(series)
    print('Augmented Dickey-Fuller Test Results:')
    print(f'ADF Statistic: {result[0]:.4f}')
    print(f'p-value: {result[1]:.4f}')

    # Interpret results
    if result[1] < 0.05:
        print("\nConclusion: The series is stationary (reject H0)")
    else:
        print("\nConclusion: The series is non-stationary (fail to reject H0)")

# Calculate and plot rolling statistics
rolling_mean = df['Load_MW'].rolling(window=4).mean()
rolling_std = df['Load_MW'].rolling(window=4).std()

plt.figure(figsize=(12, 6))
plt.plot(df['Load_MW'], label='Original')
plt.plot(rolling_mean, label='Rolling Mean')
plt.plot(rolling_std, label='Rolling Standard Deviation')
plt.title('Rolling Statistics')
plt.xlabel('Time')
plt.ylabel('Load (MW)')
plt.legend()
plt.grid(True)

# Perform ADF test
perform_adf_test(df['Load_MW'])

"""The original series (blue line) shows periodic peaks typically occurring in the middle of each year means seasonal variations in load.

---



The rolling mean (orange line) smooths out these seasonal fluctuations and provides a clearer view of the overall trend. It shows an upward trend in the load over the given time period.

---



The rolling standard deviation (green line) indicates the level of variability in the load data.Here, it appears relatively stable with some minor fluctuations, suggesting that while the load increases, the variability remains fairly consistent.

# **Model Trained Using LSTM**
"""

# Normalize the data
scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(df['Load_MW'].values.reshape(-1, 1))

# Create sequences for LSTM input
def create_sequences(data, time_steps=24):
    X, y = [], []
    for i in range(len(data) - time_steps):
        X.append(data[i:i + time_steps])
        y.append(data[i + time_steps])
    return np.array(X), np.array(y)

time_steps = 24  #use 24 hours of data to predict the next value
X, y = create_sequences(data_scaled, time_steps)

# Split into training and testing sets
# train_size = int(len(X) * 0.8)
# X_train, X_test = X[:train_size], X[train_size:]
# y_train, y_test = y[:train_size], y[train_size:]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Define the LSTM model
model = Sequential([
    LSTM(50, activation='relu', return_sequences=True, input_shape=(time_steps, 1)),#layer 50 #relu toavoid vanishing gradient
    Dropout(0.2), #20%. Dropout is a regularization technique used to prevent overfitting
    LSTM(50, activation='relu'),
    Dropout(0.2),
    Dense(1)  # Single output value fully connected layer
]) #2 layers for more capture complex pattern

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='mse') #MSE is commonly used for regression tasks

# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=1)

# Evaluate the model
loss = model.evaluate(X_test, y_test, verbose=0)
print(f'Test Loss: {loss}')

# Predict on the test set
y_pred = model.predict(X_test)

# Rescale predictions back to original scale
y_pred_rescaled = scaler.inverse_transform(y_pred)
y_test_rescaled = scaler.inverse_transform(y_test)

"""Training Loss and Validation Loss decreased consistently over epochs, indicates good learning and minimal overfitting"""



"""**Evaluation**"""

# Calculate evaluation metrics for the predictions
mae = mean_absolute_error(y_test_rescaled, y_pred_rescaled)
mse = mean_squared_error(y_test_rescaled, y_pred_rescaled)
rmse = np.sqrt(mse)
mape = np.mean(np.abs((y_test_rescaled - y_pred_rescaled) / y_test_rescaled)) * 100
r2 = r2_score(y_test_rescaled, y_pred_rescaled)

# Print the evaluation results
print(f"Evaluation Metrics:")
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"Mean Absolute Percentage Error (MAPE): {mape:.2f}%")
print(f"R-Squared (R²): {r2:.2f}")

"""*   Low errors and a high 𝑅^2 value reflect that the model fits the data exceptionally well
*  Minimal difference between training and validation loss suggests the model generalizes well to unseen data.

# **Anomaly Detection**

2 types



1.   **IQR Method**
2.  **Z score Method**

**IQR**
"""

def iqr_anomaly_detection(df, multiplier=1.5):

    # Calculate IQR
    Q1 = df['Load_MW'].quantile(0.25)
    Q3 = df['Load_MW'].quantile(0.75)
    IQR = Q3 - Q1

    # Calculate bounds
    lower_bound = Q1 - multiplier * IQR
    upper_bound = Q3 + multiplier * IQR

    # Detect anomalies
    anomalies = df[(df['Load_MW'] < lower_bound) | (df['Load_MW'] > upper_bound)].copy() #or

    # Print statistics
    print("\nIQR Method Statistics:")
    print(f"Q1: {Q1:.2f}")
    print(f"Q3: {Q3:.2f}")
    print(f"IQR: {IQR:.2f}")
    print(f"Lower Bound: {lower_bound:.2f}")
    print(f"Upper Bound: {upper_bound:.2f}")
    print(f"Number of anomalies detected: {len(anomalies)}")

    # Plot
    plt.figure(figsize=(12, 6))
    plt.plot(df.index, df['Load_MW'], 'b-', label='Power Load', alpha=0.7)
    plt.axhline(y=upper_bound, color='r', linestyle='--', label='Upper Bound')
    plt.axhline(y=lower_bound, color='r', linestyle='--', label='Lower Bound')

    # Plot anomalies
    if len(anomalies) > 0:
        plt.scatter(anomalies.index, anomalies['Load_MW'],
                   color='red', s=10, label='Anomalies') #size

    plt.title('Power Load Anomalies - IQR Method')
    plt.xlabel('Datetime')
    plt.ylabel('Load (MW)')
    plt.legend()
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

    return anomalies
iqr_anomaly_detection(df, multiplier=1.5)

"""**Z score**"""

def zscore_anomaly_detection(df, threshold=3):
    # Calculate z-scores
    z_scores = np.abs(stats.zscore(df['Load_MW']))

    # Calculate bounds in original scale
    mean = df['Load_MW'].mean()
    std = df['Load_MW'].std()
    lower_bound = mean - threshold * std
    upper_bound = mean + threshold * std

    # Detect anomalies
    anomalies = df[z_scores > threshold].copy()

    # Print statistics
    print("\nZ-Score Method Statistics:")
    print(f"Mean: {mean:.2f}")
    print(f"Standard Deviation: {std:.2f}")
    print(f"Lower Bound: {lower_bound:.2f}")
    print(f"Upper Bound: {upper_bound:.2f}")
    print(f"Number of anomalies detected: {len(anomalies)}")

    # Create figure with two subplots
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))

    # Plot 1: Original data with anomalies
    ax1.plot(df.index, df['Load_MW'], 'b-', label='Power Load', alpha=0.7)
    ax1.axhline(y=upper_bound, color='g', linestyle='--', label='Upper Bound')
    ax1.axhline(y=lower_bound, color='g', linestyle='--', label='Lower Bound')

    if len(anomalies) > 0:
        ax1.scatter(anomalies.index, anomalies['Load_MW'],
                   color='red', s=10, label='Anomalies')

    ax1.set_title('Power Load Anomalies - Z-Score Method')
    ax1.set_xlabel('Datetime')
    ax1.set_ylabel('Load (MW)')
    ax1.legend()
    ax1.tick_params(axis='x', rotation=45)

    # Plot 2: Z-scores
    ax2.plot(df.index, z_scores, 'b-', label='|Z-score|', alpha=0.7)
    ax2.axhline(y=threshold, color='r', linestyle='--',
                label=f'Threshold (z={threshold})')

    if len(anomalies) > 0:
        ax2.scatter(anomalies.index, z_scores[z_scores > threshold],
                   color='red', s=10, label='Anomalies')

    ax2.set_title('Absolute Z-Scores Over Time')
    ax2.set_xlabel('Datetime')
    ax2.set_ylabel('|Z-score|')
    ax2.legend()
    ax2.tick_params(axis='x', rotation=45)

    plt.tight_layout()
    plt.show()

    return anomalies
zscore_anomaly_detection(df, threshold=3)

"""The top plot visualizes the power load data over time, highlighting anomalies detected using the Z-score method.

---

The bottom plot provides a detailed view of the absolute Z-scores over time, clearly showing when the power load values deviate significantly from the mean.

**BOX PLOT**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Create box plot with outlier points
plt.figure(figsize=(12, 6))

# Create primary box plot
box_plot = plt.boxplot(df['Load_MW'],
                      patch_artist=True,
                      medianprops={'color': 'black'},
                      boxprops={'facecolor': 'lightblue'},
                      flierprops={'marker': 'o', 'markerfacecolor': 'red', 'markersize': 10})

# Calculate outliers using IQR method
Q1 = df['Load_MW'].quantile(0.25)
Q3 = df['Load_MW'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Identify outliers
outliers = df[(df['Load_MW'] < lower_bound) | (df['Load_MW'] > upper_bound)]

# Add title and labels
plt.title('Box Plot of Power Load with Outliers', fontsize=14)
plt.ylabel('Load (MW)', fontsize=12)

# Print statistical summary
print("\nOutlier Analysis Summary:")
print(f"Q1 (25th percentile): {Q1:.2f}")
print(f"Q3 (75th percentile): {Q3:.2f}")
print(f"IQR: {IQR:.2f}")
print(f"Lower bound: {lower_bound:.2f}")
print(f"Upper bound: {upper_bound:.2f}")
print(f"\nNumber of outliers detected: {len(outliers)}")

# Print outlier details if any found
if len(outliers) > 0:
    print("\nOutlier Details:")
    print(outliers[['Load_MW']].to_string())

# Create a more detailed visualization with scatter points
plt.figure(figsize=(15, 6))

# Create subplot for box plot and scatter plot combination
plt.subplot(1, 2, 1)
sns.boxplot(y=df['Load_MW'], color='lightblue')
plt.title('Box Plot of Power Load', fontsize=12)
plt.ylabel('Load (MW)')

# Create scatter plot of the time series with outliers highlighted
plt.subplot(1, 2, 2)
plt.scatter(df.index, df['Load_MW'], c='blue', alpha=0.5, label='Normal Points')
plt.scatter(outliers.index, outliers['Load_MW'], c='red', label='Outliers')
plt.title('Time Series with Outliers Highlighted', fontsize=12)
plt.ylabel('Load (MW)')
plt.xticks(rotation=45)
plt.legend()

plt.tight_layout()

# Additional analysis of outliers
if len(outliers) > 0:
    print("\nOutlier Statistics:")
    print(outliers['Load_MW'].describe())

    print("\nTime Distribution of Outliers:")
    for idx, row in outliers.iterrows():
        print(f"Time: {idx}, Load: {row['Load_MW']:.2f} MW")

"""**The Use of Anomaly Detection in This Dataset**
---


1. Anomaly detection in the PJM hourly load dataset has significant real-world applications, especially in the  energy management.

2.   Anomalies often causes critical incidents, such as equipment failures. Early detection helps prevent major disruptions.
*Example*: A sudden drop in load might signal a power plant issue or transmission failure.
3.  Customer Behavior Analysis:
Anomalies can reflect significant shifts in customer behavior, like during holidays, natural disasters, or special events.

## **Conclusion on this Task**

Time Series Analysis Findings:

---

The data shows seasonal patterns in electricity consumption with peaks (typically occurring in the middle of each year).
There is upward trend in electricity load over time, indicating use of energy is increasing day by day.
Daily patterns revealed high loads during afternoon and evening hours  and low  usage in early morning hours
The dataset was stationary according to the ADF test.The presence of seasonality is shown through ACF plot.

Anomaly Detection:

---
I implemented multiple methods (IQR, Z-score, Box Plot) to detect unusual load patterns.IQR method identified extreme outliers beyond 1.5 times the interquartile range.Z-score method detected points deviating more than 3 standard deviations from the mean.Box plots provided visual confirmation of outliers and their distribution
"""